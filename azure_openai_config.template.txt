# Azure OpenAI Training Data Generator Configuration Template
# Copy this file to azure_openai_config.txt and update with your values

## Authentication Setup
# Using Azure AD authentication (no API key needed):

# 1. Login to Azure CLI
az login

# 2. Set environment variables with your Azure OpenAI resource details
export AZURE_OPENAI_ENDPOINT="https://your-resource-name.openai.azure.com/"
export AZURE_OPENAI_DEPLOYMENT="gpt-4o"

## Usage Examples

# Basic usage with 500 files (recommended for full coverage)
python3 code-scanner/generate_azure_openai_training_data.py \
  --repo-path /workspace/upstream/containerd \
  --output-path output/containerd_training_data_azure.jsonl \
  --max-files 500 \
  --max-qa-per-file 12

# Large scale processing (for comprehensive training data)
python3 code-scanner/generate_azure_openai_training_data.py \
  --repo-path /workspace/upstream/containerd \
  --output-path output/containerd_training_data_large.jsonl \
  --max-files 1000 \
  --max-qa-per-file 15

# Cost-optimized processing (for budget-conscious scenarios)
python3 code-scanner/generate_azure_openai_training_data.py \
  --repo-path /workspace/upstream/containerd \
  --output-path output/containerd_training_data_cost_optimized.jsonl \
  --max-files 200 \
  --max-qa-per-file 8

## Expected Output
# With 500 files × 10 Q&A pairs average = ~5,000 training examples
# With 1000 files × 12 Q&A pairs average = ~12,000 training examples

## Cost Estimation (approximate)
# GPT-4o: ~$0.005 per 1K tokens input, ~$0.015 per 1K tokens output  
# Average file analysis: ~2K tokens input, ~1K tokens output = ~$0.025 per file
# 500 files: ~$12.50
# 1000 files: ~$25.00

## Quality Benefits
# - Deep code analysis by GPT-4o
# - Contextual understanding of containerd architecture
# - Diverse question types (technical, practical, architectural)
# - High-quality answers with proper technical depth
# - Better coverage of edge cases and integration points
# - Azure AD authentication (no API key management needed)

## GitHub Issues Analysis Examples

# Fetch GitHub issues (prioritized by bugs, questions, maintainer responses)
python3 issue-miner/prioritize_github_issues.py \
  --max-issues 500 \
  --output-path output/github_issues_metadata.json

# Generate training data from issues
python3 issue-miner/generate_issue_training_data.py \
  --issues-metadata output/github_issues_metadata.json \
  --output-path output/github_issues_training_data.jsonl \
  --max-issues 100 \
  --max-issues-per-minute 6

# Combined approach (recommended for comprehensive training)
python3 issue-miner/prioritize_github_issues.py --max-issues 200
python3 issue-miner/generate_issue_training_data.py --max-issues 100
